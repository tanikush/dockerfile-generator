# ğŸ³ Dockerfile Generator using Local LLM

Automate Dockerfile generation for any project using locally hosted LLM (Ollama). This tool automatically creates optimized Dockerfiles based on your project specifications - completely FREE and offline!

## ğŸŒŸ Features

- âœ… **Automatic Project Detection** - Detects Python, Node.js, Java, and Go projects
- âœ… **AI-Powered Generation** - Uses Llama3 via Ollama for intelligent Dockerfile creation
- âœ… **100% FREE** - No API costs, runs completely offline
- âœ… **Best Practices** - Generates production-ready Dockerfiles with optimization
- âœ… **Zero Configuration** - Just point to your project and go!

## ğŸ› ï¸ Tech Stack

- **Ollama** - Local LLM runtime (FREE)
- **Llama3** - AI model for Dockerfile generation
- **Python** - Automation scripting
- **Docker** - Container platform

## ğŸ“‹ Prerequisites

- Python 3.7+
- Ollama installed
- 8GB+ RAM recommended
- 5GB+ free disk space (for Llama3 model)

## ğŸš€ Installation

### Step 1: Install Ollama

**Windows:**
```bash
# Download from https://ollama.com/download
# Run OllamaSetup.exe
```

**Mac/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Step 2: Download Llama3 Model

```bash
ollama pull llama3
```

### Step 3: Clone & Setup Project

```bash
git clone https://github.com/yourusername/dockerfile-generator.git
cd dockerfile-generator
pip install -r requirements.txt
```

## ğŸ’» Usage

### Basic Usage

```bash
python main.py /path/to/your/project
```

### Interactive Mode

```bash
python main.py
# Enter project path when prompted
```

### Example

```bash
python main.py C:\Users\YourName\my-flask-app
```

**Output:**
```
==================================================
ğŸ³ Dockerfile Generator using Local LLM
==================================================

ğŸ“‚ Analyzing project: C:\Users\YourName\my-flask-app
âœ… Detected: Python project
ğŸ“¦ Dependencies: 3 found

ğŸ¤– Generating Dockerfile using Ollama (llama3)...

âœ… Dockerfile generated successfully!
ğŸ“ Saved to: generated\Dockerfile
```

## ğŸ“ Project Structure

```
dockerfile-generator/
â”œâ”€â”€ main.py              # Main entry point
â”œâ”€â”€ analyzer.py          # Project type detection
â”œâ”€â”€ llm_handler.py       # Ollama LLM integration
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ generated/           # Generated Dockerfiles
â”œâ”€â”€ README.md           # This file
â””â”€â”€ CODE_EXPLANATION.md # Detailed code explanation
```

## ğŸ¯ Supported Project Types

| Project Type | Detection File | Status |
|-------------|----------------|--------|
| Python | `requirements.txt` | âœ… Supported |
| Node.js | `package.json` | âœ… Supported |
| Java | `pom.xml` | âœ… Supported |
| Go | `go.mod` | âœ… Supported |

## ğŸ“– How It Works

1. **Analyze** - Scans project directory for language-specific files
2. **Detect** - Identifies project type and dependencies
3. **Generate** - Sends context to Llama3 via Ollama
4. **Optimize** - AI creates production-ready Dockerfile
5. **Save** - Outputs to `generated/Dockerfile`

## ğŸ“ What You'll Learn

- Container automation fundamentals
- CI/CD pipeline basics
- LLM integration in real-world applications
- Docker best practices
- Python automation scripting

## ğŸ”§ Configuration

The tool works out-of-the-box with sensible defaults. To customize:

- **Change LLM Model**: Edit `llm_handler.py` â†’ `model='llama3'`
- **Add Project Types**: Edit `analyzer.py` â†’ Add detection logic
- **Modify Prompts**: Edit `llm_handler.py` â†’ Update prompt template

## ğŸ› Troubleshooting

### Ollama not found
```bash
# Restart terminal after installation
# Or add to PATH manually
```

### Model not downloaded
```bash
ollama pull llama3
```

### Connection error
```bash
# Start Ollama service
ollama serve
```

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Add support for more languages
- Improve prompt engineering
- Enhance error handling
- Add tests

## ğŸ“ License

MIT License - Feel free to use in your projects!

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.com/) - Local LLM runtime
- [Meta AI](https://ai.meta.com/) - Llama3 model
- Docker community for best practices

## ğŸ“§ Contact

For questions or suggestions, open an issue on GitHub!

---

**â­ If this project helped you, please star it on GitHub!**

Made with â¤ï¸ by Tanisha Kushwah
